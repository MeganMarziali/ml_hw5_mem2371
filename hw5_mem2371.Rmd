---
title: "Assignment 5"
author: "Megan Marziali"
date: "Feb 10, 2021"
output:
  github_document: default
editor_options:
  chunk_output_type: console
---

## Part 1: Set-up

### Problem set-up

##### Loading required packages

The following code chunk loads the required packages for the assignment.
```{r packages, message = FALSE}
library(tidyverse) 
library(caret)
library(glmnet)

set.seed(100)
```

##### Loading data into environment and cleaning

```{r data_prepare, message = FALSE, results = FALSE}
alc_data = 
  read.csv("./data/alcohol_use.csv", na = c("", ".", "NA", ".d", ".r")) %>% 
  janitor::clean_names() %>%
  mutate(
    alc_consumption = as.factor(alc_consumption)
  ) %>% 
  select(-x)

summary(alc_data)
```

To clean the data, I stripped off the ID variable and recoded the alcohol consumption variable as a factor. The complete dataset includes 1885 observations. No missing observations are present within the data.

##### Data partitioning

```{r partition, message = FALSE}
train.indices = createDataPartition(y = alc_data$alc_consumption,p = 0.7,list = FALSE)

training = alc_data[train.indices,]
testing = alc_data[-train.indices,]

#Store outcome 
alc_train = training$alc_consumption
alc_test = testing$alc_consumption

#Model.matrix shortcut to removing outcome variable from matrix
x_train = model.matrix(alc_consumption~., training)[,-1] # Check what the [-1] does
x_test = model.matrix(alc_consumption~., testing)[,-1]

# Convert the outcome (class) to a numerical variable for glmnet
alc_c = ifelse(training$alc_consumption == "CurrentUse", 1, 0)
```

## Problem 1

### 1.1: Choosing alpha and lambda via cross-validation

```{r}
model.1 = train(
  alc_consumption ~., data = training, method = "glmnet",
  trControl = trainControl("cv", number = 10),
 tuneLength = 10
  )

param = model.1$bestTune

# Model coefficients
coef(model.1$finalModel, model.1$bestTune$lambda) # ask about final model, where is this coming from?

# Make predictions ==== evaluating

en.pred <- model.1 %>% predict(x_test)

#model.1.train.final = glmnet(x_train, alc_train, alpha = param$alpha, lambda = param$lambda, family = binomial)
#coef(model.1.train.final)

#model_1 = glmnet(x_train, alc_c, family = binomial, alpha = param$alpha, lambda = param$lambda)
#coef(model_1)
```

### 1.2: Model that uses logistic regression

```{r}
model_2 = glm(alc_consumption ~ . ,family = binomial(link = 'logit'),data = training)
summary(model_2)
```

### 1.3: LASSO model using all features

```{r}
model_3 = cv.glmnet(x_train, alc_train, alpha = 1, standardize = TRUE, family = binomial)

plot(model_3)
model_3$lambda.min
model_3$lambda.1se

model_3_final = glmnet(x_train, alc_train, alpha = 1, lambda = model_3$lambda.1se, family = binomial)
coef(model_3_final)
```

## Part 2: Evaluating Model Performance

### 2.1: Alpha and lambda cross-validation model

```{r}
fitted.results = predict(model_1, x_test, type = 'response') 
fitted.results.p = ifelse(fitted.results > 0.5, 1, 0)

testing.outcome = (as.numeric(testing$alc_consumption) - 1)

model.1.test.pred = model_1 %>% predict(x_test) %>% as.vector()

misClasificError = mean(model.1.test.pred != testing$alc_consumption, na.rm = T)
print(paste('Accuracy Model 1',1 - misClasificError))

#testProbs <- data.frame(obs = testing$alc_consumption,
#                        pred.logit = fitted.results)
#calPlotData = calibration(obs ~ pred.logit, 
#                          data = testProbs, 
#                          class = "NotCurrentUse", 
#                          cuts = 5)

# Make predictions on the test data
probabilities = model_1 %>% predict(newx = x_test)
predicted.classes = ifelse(probabilities > 0.5, "CurrentUse", "NotCurrentUse")

# Model accuracy
observed.classes = testing$alc_c
mean(predicted.classes == observed.classes)
```

### 2.2: Evaluating logistic regression model

```{r}
fitted.results.2 = predict(model.2, testing, type = 'response') 
fitted.results.p.2 = ifelse(fitted.results > 0.5,1,0)

testing.outcome.2 = (as.numeric(testing$alc_consumption) - 1)

misClasificError.2 = mean(fitted.results.p != testing.outcome.2, na.rm = T)
print(paste('Accuracy Model 2',1 - misClasificError.2))

testProbs.2 = data.frame(obs = testing$alc_consumption,
                        pred.logit.2 = fitted.results.2)

calPlotData.2 = calibration(obs ~ pred.logit.2, 
                          data = testProbs.2, 
                          class = "NotCurrentUse", 
                          cuts = 5)

xyplot(calPlotData.2, auto.key = list(columns = 2))

# Make predictions
probabilities.2 = model.2 %>% predict(testing, type = "response")
predicted.classes.2 = ifelse(probabilities > 0.5, "CurrentUse", "NotCurrentUse")
# Model accuracy
observed.classes.2 = testing$alc_consumption
mean(predicted.classes.2 == observed.classes.2)
```

```{r}
fitted.results.3 = predict(model.3, testing, type = 'response') 
fitted.results.p.3 = ifelse(fitted.results > 0.5, 1, 0)

testing.outcome.3 = (as.numeric(testing$alc_consumption) - 1)

misClasificError.3 = mean(fitted.results.p != testing.outcome.3, na.rm = T)
print(paste('Accuracy Model 3',1 - misClasificError.3))

# Making predictions
probabilities = model.3 %>% predict(newx = x_test)
predicted.classes = ifelse(probabilities > 0.5, "CurrentUse", "NotCurrentUse")
# Model accuracy
observed.classes = testing$alc_consumption
mean(predicted.classes == observed.classes)
```


